{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NpttnExP8Az",
        "outputId": "e1a55854-b1f3-4f24-81e0-7bd479365ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/73.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylcs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pylcs -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPisqftKTsa1",
        "outputId": "89d8d7b9-b857-409f-f70d-86b301d6c98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue8u0uipTuMr",
        "outputId": "ede6518c-c9d9-4e6b-d8d6-d8a20c6e8061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Low Resource\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/Low Resource\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdkVINIYSprZ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import pylcs\n",
        "import openai.error\n",
        "import uuid\n",
        "import csv\n",
        "import glob\n",
        "import openai.error\n",
        "import helpers\n",
        "import difflib\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKURmHloP8A2"
      },
      "outputs": [],
      "source": [
        "openai.api_key = 'INSERT KEY HERE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96cCujF2Undo"
      },
      "outputs": [],
      "source": [
        "with open('COVID-QA.json','r') as f:\n",
        "  full_data = json.load(f)[\"data\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P77kaICxUnlK"
      },
      "outputs": [],
      "source": [
        "def find_in_context(context, substring):\n",
        "  substring_lower = substring.lower()\n",
        "  context_lower = context.lower()\n",
        "  start = \" \".join(substring_lower.split()[0:3])\n",
        "  end = \" \".join(substring_lower.split()[-3:])\n",
        "  start_idx = context_lower.find(start)\n",
        "  end_idx = context_lower.find(end)\n",
        "\n",
        "  if start_idx != -1 and end_idx != -1:\n",
        "    return context[start_idx:end_idx + len(end)]\n",
        "\n",
        "  return \"\"\n",
        "\n",
        "def process_answers(new_context,curr_answers):\n",
        "  answers = []\n",
        "  for i,answer in enumerate(curr_answers):\n",
        "    if new_context.find(answer) != -1:\n",
        "      answers.append(answer)\n",
        "    elif new_context.find(answer[:-1]) != -1:\n",
        "      answers.append(answer[:-1])\n",
        "    elif new_context.find(answer[0].lower() + answer[1:]) != -1:\n",
        "      answers.append(answer[0].lower() + answer[1:])\n",
        "    elif new_context.find(answer[0].lower() + answer[1:-1]) != -1:\n",
        "      answers.append(answer[0].lower() + answer[1:-1])\n",
        "    elif find_in_context(new_context, answer) != \"\":\n",
        "      answers.append(find_in_context(new_context, answer))\n",
        "    elif find_in_context(new_context, answer[:-1]) != \"\":\n",
        "      answers.append(find_in_context(new_context, answer[:-1]))\n",
        "    else:\n",
        "      answers.append(\"\")\n",
        "\n",
        "  return answers\n",
        "\n",
        "\n",
        "def find_answer_context(context, answer, max_context_length=500, num_sentences_before=3, num_sentences_after=4):\n",
        "    # Strip leading and trailing whitespaces from the answer\n",
        "    answer = answer.strip()\n",
        "\n",
        "    # Find the sentence that contains the answer\n",
        "    if not isinstance(context,str):\n",
        "      return \"\"\n",
        "    context_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', context)\n",
        "    target_sentence_index = next((i for i, s in enumerate(context_sentences) if answer.lower() in s.lower()), None)\n",
        "\n",
        "    # If no sentence contains the answer, return None\n",
        "    if target_sentence_index is None:\n",
        "        return None\n",
        "\n",
        "    # Determine the start and end indices of the context window\n",
        "    context_start = max(0, target_sentence_index - num_sentences_before)\n",
        "    context_end = min(len(context_sentences), target_sentence_index + num_sentences_after + 1)\n",
        "\n",
        "    # Extract the answer-containing context\n",
        "    answer_context = ' '.join(context_sentences[context_start:context_end])\n",
        "\n",
        "    return answer_context\n",
        "\n",
        "def process_string(input_string):\n",
        "    input_string = input_string.strip(\"\\n\")\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', input_string)\n",
        "\n",
        "    # Calculate the index to extract the bottom 50% of sentences\n",
        "    bottom_half_index = 3 * len(sentences) // 4\n",
        "\n",
        "    # Extract the bottom 50% of sentences\n",
        "    bottom_half_sentences = sentences[bottom_half_index:]\n",
        "\n",
        "    # Join the sentences back into a single string without spaces between sentences\n",
        "    result = ' '.join(bottom_half_sentences)\n",
        "\n",
        "    return result\n",
        "\n",
        "def extract_qa_pairs(qa):\n",
        "    # Regular expression patterns to capture questions and answers\n",
        "    question_pattern = r\"(?:Question\\s*:\\s*|Question\\s*\\d+\\s*:\\s*)(.*?)(?=\\n\\s*(?:Question\\s*:\\s*|Question\\s*\\d+\\s*:|Answer\\s*:\\s*|Answer\\s*\\d+\\s*:|$))\"\n",
        "    answer_pattern = r\"(?:Answer\\s*:\\s*|Answer\\s*\\d+\\s*:\\s*)(.*?)(?=\\n\\s*(?:Question\\s*:\\s*|Question\\s*\\d+\\s*:|Answer\\s*:\\s*|Answer\\s*\\d+\\s*:|$))\"\n",
        "\n",
        "    # Extract questions and answers using regular expressions\n",
        "    questions = re.findall(question_pattern, qa, flags=re.DOTALL)\n",
        "    answers = re.findall(answer_pattern, qa, flags=re.DOTALL)\n",
        "\n",
        "    return questions, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZjPewcoYFw2"
      },
      "outputs": [],
      "source": [
        "context_multiplier = 4\n",
        "qa_per_context = 20\n",
        "total_articles = 116\n",
        "starting = 85\n",
        "curr_file = \"COVID_AUGMENT_SMALL_TWO_SHOT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXW-0iptN38K"
      },
      "outputs": [],
      "source": [
        "with open('COVID_AUGMENT_SMALL_TWO_SHOT.json','r') as f:\n",
        "  dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jrjU0Ckqk0P"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import openai\n",
        "\n",
        "def make_openai_request(prompt: str, model: str, max_tokens=8000):\n",
        "    max_retry_count = 10\n",
        "    retry_count = 1\n",
        "    retry_time_sec = 30\n",
        "\n",
        "    while retry_count <= max_retry_count:\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model, messages=prompt)\n",
        "            answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "            # answer = answer.strip().strip('\"')\n",
        "            return answer\n",
        "        except openai.error.APIConnectionError as e:\n",
        "            print(\n",
        "                f\"API connection error: {e}.\\nRetrying in {retry_time_sec}...\")\n",
        "        except openai.error.RateLimitError as e:\n",
        "            print(\n",
        "                f\"Rate limit error: {e}.\\nRetrying in {retry_time_sec}...\")\n",
        "        except openai.error.Timeout as e:\n",
        "            print(f\"Timeout error: {e}.\\nRetrying in {retry_time_sec}...\")\n",
        "        except openai.error.ServiceUnavailableError as e:\n",
        "            print(\n",
        "                f\"ServiceUnavailable error: {e}.\\nRetrying in {retry_time_sec}...\")\n",
        "        except OSError as e:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            %cd drive/MyDrive/Low Resource\n",
        "        except openai.error.OpenAIError as e:\n",
        "            print(f\"Unhandled OpenAI error occurred: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unhandled error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "        retry_count += 1\n",
        "        time.sleep(retry_time_sec)\n",
        "\n",
        "    if retry_count > max_retry_count:\n",
        "        print(\n",
        "            f\"Retries of {retry_count} exceeded max_retry_count of {max_retry_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDCO8K-5kxuA"
      },
      "outputs": [],
      "source": [
        "def get_two_shot(dataset):\n",
        "    two_shot = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for i, data in enumerate(dataset[\"data\"]):\n",
        "      if i == len(dataset[\"data\"]) - 1:\n",
        "        break\n",
        "      data2 = dataset[\"data\"][i+1]\n",
        "      context1 = data[\"paragraphs\"][0][\"context\"]\n",
        "      question1 = data[\"paragraphs\"][0][\"qas\"][0][\"question\"]\n",
        "      answer1 = data[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"]\n",
        "\n",
        "      context2 = data2[\"paragraphs\"][0][\"context\"]\n",
        "      question2 = data2[\"paragraphs\"][0][\"qas\"][0][\"question\"]\n",
        "      answer2 = data2[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"]\n",
        "\n",
        "      two_shot.append((context1, question1, answer1, context2, question2, answer2))\n",
        "\n",
        "    return two_shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ty0iJh0Nz-S"
      },
      "outputs": [],
      "source": [
        "two_shot = get_two_shot(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip11WR6alhM4",
        "outputId": "4745c6f1-1754-48ec-b848-a5e44d50e668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On document 86/116\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 87/116\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 88/116\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 89/116\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->13 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 90/116\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 91/116\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 92/116\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 93/116\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->5 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 94/116\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->10 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 95/116\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->21 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 96/116\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->8 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 97/116\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 98/116\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 99/116\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 100/116\n",
            "    -------->18 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->13 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 101/116\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->13 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 102/116\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 103/116\n",
            "    -------->8 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 104/116\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 105/116\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->18 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->15 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 106/116\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->18 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 107/116\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->10 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 108/116\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 109/116\n",
            "    -------->13 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->7 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 110/116\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 111/116\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->20 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 112/116\n",
            "    -------->8 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->12 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 113/116\n",
            "    -------->17 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 114/116\n",
            "    -------->16 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->13 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 115/116\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->14 answers kept\n",
            "    -------> Saved to Json\n",
            "On document 116/116\n",
            "    -------->18 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->19 answers kept\n",
            "    -------> Saved to Json\n",
            "    -------->11 answers kept\n",
            "    -------> Saved to Json\n"
          ]
        }
      ],
      "source": [
        "for cnt in range(starting,total_articles):\n",
        "  print(f'On document {cnt + 1}/{total_articles}')\n",
        "  example_context1,example_question1,example_answer1,example_context2,example_question2,example_answer2 = two_shot[cnt]\n",
        "  short_context1 = process_string(example_context1)\n",
        "  short_context2 = process_string(example_context2)\n",
        "\n",
        "  for i in range(context_multiplier - 1):\n",
        "    new_dict = {\"qas\":[],\"context\":\"\"}\n",
        "    context_prompt = [\n",
        "                      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": f\"Generate a context that is similar in topic and domain distribution to the given two context. Example Context:{short_context1}. Example Context:{short_context2} Context:\"}\n",
        "                      ]\n",
        "    generated_context = make_openai_request(context_prompt, \"gpt-4\")\n",
        "    new_dict[\"context\"] = generated_context\n",
        "    downsized_context1 = find_answer_context(example_context1,example_answer1)\n",
        "    example_context1 = downsized_context1 if downsized_context1 != \"\" else example_context1\n",
        "    downsized_context2 = find_answer_context(example_context2,example_answer2)\n",
        "    example_context2 = downsized_context2 if downsized_context2 != \"\" else example_context2\n",
        "\n",
        "\n",
        "    qa_prompt = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Based only on the given context generate 1 question answer pair. The answer must be only made up of substrings from the context and do not generate any new text for the answer. Context:{example_context1}.\"},\n",
        "                    {\"role\": \"assistant\", \"content\": f\"Question: {example_question1} Answer: {example_answer1}\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Based only on the given context generate 1 question answer pair. The answer must be only made up of substrings from the context and do not generate any new text for the answer. Context:{example_context2}.\"},\n",
        "                    {\"role\": \"assistant\", \"content\": f\"Question: {example_question2} Answer: {example_answer2}\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Based only on the given context generate {qa_per_context} question answer pair. The answer must be only made up of substrings from the context and do not generate any new text for the answer. Context:{generated_context}.\"}\n",
        "                ]\n",
        "    qa = make_openai_request(qa_prompt, \"gpt-4\")\n",
        "    if not qa:\n",
        "      continue\n",
        "    if len(qa) > 0 and qa[-1] != \"\\n\":\n",
        "      qa = qa + \"\\n\"\n",
        "    questions,answers = extract_qa_pairs(qa)\n",
        "    answers = process_answers(generated_context,answers)\n",
        "    print(f'    -------->{len([elem for elem in answers if elem != \"\"])} answers kept')\n",
        "\n",
        "\n",
        "    if len(answers) != len(questions):\n",
        "      print(\"      -------> Skipped this passage\")\n",
        "      continue\n",
        "\n",
        "    for j in range(len(questions)):\n",
        "      question = questions[j]\n",
        "      answer = answers[j]\n",
        "      ques_id = str(uuid.uuid4())\n",
        "      ans_start = generated_context.find(answer)\n",
        "\n",
        "      if answer == \"\" or ans_start == -1:\n",
        "        continue\n",
        "\n",
        "      new_dict[\"qas\"].append({\"question\":question, \"id\":ques_id, \"answers\":[{\"text\":answer, \"answer_start\": ans_start}], \"is_impossible\": False})\n",
        "\n",
        "    dataset[\"data\"].append({\"paragraphs\":[new_dict]})\n",
        "    try:\n",
        "      with open(f\"{curr_file}.json\", \"w\") as f:\n",
        "        json.dump(dataset, f)\n",
        "        print(\"    -------> Saved to Json\")\n",
        "    except OSError as e:\n",
        "          from google.colab import drive\n",
        "          drive.mount('/content/drive')\n",
        "          %cd drive/MyDrive/Low Resource\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
